{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp agents.cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Agent\n",
    "> An agent for solving the Cartpole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import gym\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# OpenCog\n",
    "from opencog.logger import log\n",
    "from opencog.pln import *\n",
    "from opencog.type_constructors import *\n",
    "from opencog.utilities import set_default_atomspace\n",
    "\n",
    "# ROCCA\n",
    "from rocca.envs.wrappers import GymWrapper\n",
    "from rocca.agents import OpencogAgent\n",
    "from rocca.agents.utils import *\n",
    "\n",
    "from rocca.utils import *\n",
    "from rocca.agents.core import logger as ac_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CartPoleWrapper(GymWrapper):\n",
    "    def __init__(self, env):\n",
    "        action_list = [\"Go Left\", \"Go Right\"]\n",
    "        super().__init__(env, action_list)\n",
    "\n",
    "    def labeled_observations(self, space, obs, sbs=\"\"):\n",
    "        \"\"\"Translate gym observation to Atomese\n",
    "\n",
    "        There are 4 observations (taken from CartPoleEnv help)\n",
    "\n",
    "        Observation               Min             Max\n",
    "        -----------               ---             ---\n",
    "        Cart Position             -4.8            4.8\n",
    "        Cart Velocity             -Inf            Inf\n",
    "        Pole Angle                -24 deg         24 deg\n",
    "        Pole Velocity At Tip      -Inf            Inf\n",
    "\n",
    "        They are represented in atomese as follows\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Cart Position\"\n",
    "            Number CP\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Cart Velocity\"\n",
    "            Number CV\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Pole Angle\"\n",
    "            Number PA\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Pole Velocity At Tip\"\n",
    "            Number PVAT\n",
    "\n",
    "        Note that the observations are neither tv-set nor\n",
    "        timestamped. It is up to the caller to do it.\n",
    "\n",
    "        A python list (not an atomese list) is returned with these 4\n",
    "        Atomese observations.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        cp = NumberNode(str(obs[0]))\n",
    "        cv = NumberNode(str(obs[1]))\n",
    "        pa = NumberNode(str(obs[2]))\n",
    "        pvat = NumberNode(str(obs[3]))\n",
    "\n",
    "        return [\n",
    "            EvaluationLink(PredicateNode(\"Cart Position\"), cp),\n",
    "            EvaluationLink(PredicateNode(\"Cart Velocity\"), cv),\n",
    "            EvaluationLink(PredicateNode(\"Pole Angle\"), pa),\n",
    "            EvaluationLink(PredicateNode(\"Cart Velocity At Tip\"), pvat),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Rule Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class FixedCartPoleAgent(OpencogAgent):\n",
    "    def __init__(self, env: GymWrapper):\n",
    "        # Create Action Space. The set of allowed actions an agent can take.\n",
    "        # TODO take care of action parameters.\n",
    "        action_space = {ExecutionLink(SchemaNode(a)) for a in env.action_list}\n",
    "\n",
    "        # Create Goal\n",
    "        pgoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"1\"))\n",
    "        ngoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"0\"))\n",
    "\n",
    "        # Call super ctor\n",
    "        super().__init__(env, action_space, pgoal, ngoal)\n",
    "\n",
    "    def plan(self, goal, expiry) -> List:\n",
    "        \"\"\"Plan the next actions given a goal and its expiry time offset\n",
    "\n",
    "        Return a python list of cognitive schematics.  Whole cognitive\n",
    "        schematics are output (instead of action plans) in order to\n",
    "        make a decision based on their truth values.  Alternatively it\n",
    "        could return a pair (action plan, tv), where tv has been\n",
    "        evaluated to take into account the truth value of the context\n",
    "        as well (which would differ from the truth value of rule in\n",
    "        case the context is uncertain).\n",
    "\n",
    "        The format for a cognitive schematic is as follows\n",
    "\n",
    "        PredictiveImplicationScope <tv>\n",
    "          <vardecl>\n",
    "          <expiry>\n",
    "          And (or SimultaneousAnd?)\n",
    "            <context>\n",
    "            Execution\n",
    "              <action>\n",
    "              <input> [optional]\n",
    "              <output> [optional]\n",
    "          <goal>\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # For now we provide 2 hardwired rules\n",
    "        #\n",
    "        # 1. Push cart to the left (0) if angle is negative\n",
    "        # 2. Push cart to the right (1) if angle is positive\n",
    "        #\n",
    "        # with some arbitrary truth value (stv 0.9, 0.1)\n",
    "        angle = VariableNode(\"$angle\")\n",
    "        numt = TypeNode(\"NumberNode\")\n",
    "        time_offset = to_nat(1)\n",
    "        pole_angle = PredicateNode(\"Pole Angle\")\n",
    "        go_right = SchemaNode(\"Go Right\")\n",
    "        go_left = SchemaNode(\"Go Left\")\n",
    "        reward = PredicateNode(\"Reward\")\n",
    "        epsilon = NumberNode(\"0.01\")\n",
    "        mepsilon = NumberNode(\"-0.01\")\n",
    "        unit = NumberNode(\"1\")\n",
    "        hTV = TruthValue(0.9, 0.1)  # High TV\n",
    "        lTV = TruthValue(0.1, 0.1)  # Low TV\n",
    "\n",
    "        # PredictiveImplicationScope <high TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       Variable \"$angle\"\n",
    "        #       0\n",
    "        #     Execution\n",
    "        #       Schema \"Go Right\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_rr = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(angle, epsilon),\n",
    "                # Action\n",
    "                ExecutionLink(go_right),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=hTV,\n",
    "        )\n",
    "\n",
    "        # PredictiveImplicationScope <high TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       0\n",
    "        #       Variable \"$angle\"\n",
    "        #     Execution\n",
    "        #       Schema \"Go Left\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_ll = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(mepsilon, angle),\n",
    "                # Action\n",
    "                ExecutionLink(go_left),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=hTV,\n",
    "        )\n",
    "\n",
    "        # To cover all possibilities we shouldn't forget the complementary\n",
    "        # actions, i.e. going left when the pole is falling to the right\n",
    "        # and such, which should make the situation worse.\n",
    "\n",
    "        # PredictiveImplicationScope <low TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And (or SimultaneousAnd?)\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       Variable \"$angle\"\n",
    "        #       0\n",
    "        #     Execution\n",
    "        #       Schema \"Go Left\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_rl = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(angle, epsilon),\n",
    "                # Action\n",
    "                ExecutionLink(go_left),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=lTV,\n",
    "        )\n",
    "\n",
    "        # PredictiveImplicationScope <low TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And (or SimultaneousAnd?)\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       0\n",
    "        #       Variable \"$angle\"\n",
    "        #     Execution\n",
    "        #       Schema \"Go Right\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_lr = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(mepsilon, angle),\n",
    "                # Action\n",
    "                ExecutionLink(go_right),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=lTV,\n",
    "        )\n",
    "\n",
    "        # Ideally we want to return only relevant cognitive schematics\n",
    "        # (i.e. with contexts probabilistically currently true) for\n",
    "        # now however we return everything and let to the deduction\n",
    "        # process deal with it, as it should be able to.\n",
    "        return [cs_ll, cs_rr, cs_rl, cs_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class LearningCartPoleAgent(OpencogAgent):\n",
    "    def __init__(self, env: GymWrapper, log_level=\"debug\"):\n",
    "        # Create Action Space. The set of allowed actions an agent can take.\n",
    "        # TODO take care of action parameters.\n",
    "        action_space = {ExecutionLink(SchemaNode(a)) for a in env.action_list}\n",
    "\n",
    "        # Create Goal\n",
    "        pgoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"1\"))\n",
    "        ngoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"0\"))\n",
    "\n",
    "        # Call super ctor\n",
    "        super().__init__(env, action_space, pgoal, ngoal, log_level=log_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(2)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomspace = AtomSpace()\n",
    "set_default_atomspace(atomspace)\n",
    "wrapped_env = CartPoleWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| msg: 'The final reward is 40.'\n"
     ]
    }
   ],
   "source": [
    "# tb_writer = SummaryWriter(comment=\"cartpole-fixed\")\n",
    "\n",
    "cpa = FixedCartPoleAgent(wrapped_env)\n",
    "cpa.delta = 1.0e-16\n",
    "\n",
    "# Run control loop\n",
    "while not cpa.step():\n",
    "    time.sleep(0.1)\n",
    "    log.debug(\"step_count = {}\".format(cpa.step_count))\n",
    "    # tb_writer.add_scalar(\n",
    "    #     \"accumulated_reward\", cpa.accumulated_reward, cpa.step_count\n",
    "    # )\n",
    "\n",
    "log_msg(agent_log, f\"The final reward is {cpa.accumulated_reward}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<opencog.atomspace.AtomSpace at 0x7f7dd0fe9dc0>"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpa.atomspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomspace = AtomSpace()\n",
    "set_default_atomspace(atomspace)\n",
    "wrapped_env = CartPoleWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((PredictiveImplicationScopeLink (stv 0.1 0.1)\n",
      "  (TypedVariableLink\n",
      "    (VariableNode \"$angle\")\n",
      "    (TypeNode \"NumberNode\"))\n",
      "  (SLink\n",
      "    (ZLink))\n",
      "  (AndLink\n",
      "    (ExecutionLink\n",
      "      (SchemaNode \"Go Right\"))\n",
      "    (EvaluationLink\n",
      "      (PredicateNode \"Pole Angle\")\n",
      "      (VariableNode \"$angle\"))\n",
      "    (GreaterThanLink\n",
      "      (NumberNode \"-0.01\")\n",
      "      (VariableNode \"$angle\")))\n",
      "  (EvaluationLink\n",
      "    (PredicateNode \"Reward\")\n",
      "    (NumberNode \"1\")))\n",
      " (PredictiveImplicationScopeLink (stv 0.9 0.1)\n",
      "  (TypedVariableLink\n",
      "    (VariableNode \"$angle\")\n",
      "    (TypeNode \"NumberNode\"))\n",
      "  (SLink\n",
      "    (ZLink))\n",
      "  (AndLink\n",
      "    (EvaluationLink\n",
      "      (PredicateNode \"Pole Angle\")\n",
      "      (VariableNode \"$angle\"))\n",
      "    (ExecutionLink\n",
      "      (SchemaNode \"Go Left\"))\n",
      "    (GreaterThanLink\n",
      "      (NumberNode \"-0.01\")\n",
      "      (VariableNode \"$angle\")))\n",
      "  (EvaluationLink\n",
      "    (PredicateNode \"Reward\")\n",
      "    (NumberNode \"1\")))\n",
      " (PredictiveImplicationScopeLink (stv 0.9 0.1)\n",
      "  (TypedVariableLink\n",
      "    (VariableNode \"$angle\")\n",
      "    (TypeNode \"NumberNode\"))\n",
      "  (SLink\n",
      "    (ZLink))\n",
      "  (AndLink\n",
      "    (ExecutionLink\n",
      "      (SchemaNode \"Go Right\"))\n",
      "    (EvaluationLink\n",
      "      (PredicateNode \"Pole Angle\")\n",
      "      (VariableNode \"$angle\"))\n",
      "    (GreaterThanLink\n",
      "      (VariableNode \"$angle\")\n",
      "      (NumberNode \"0.01\")))\n",
      "  (EvaluationLink\n",
      "    (PredicateNode \"Reward\")\n",
      "    (NumberNode \"1\")))\n",
      " (PredictiveImplicationScopeLink (stv 0.1 0.1)\n",
      "  (TypedVariableLink\n",
      "    (VariableNode \"$angle\")\n",
      "    (TypeNode \"NumberNode\"))\n",
      "  (SLink\n",
      "    (ZLink))\n",
      "  (AndLink\n",
      "    (EvaluationLink\n",
      "      (PredicateNode \"Pole Angle\")\n",
      "      (VariableNode \"$angle\"))\n",
      "    (GreaterThanLink\n",
      "      (VariableNode \"$angle\")\n",
      "      (NumberNode \"0.01\"))\n",
      "    (ExecutionLink\n",
      "      (SchemaNode \"Go Left\")))\n",
      "  (EvaluationLink\n",
      "    (PredicateNode \"Reward\")\n",
      "    (NumberNode \"1\")))\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from opencog.scheme import scheme_eval, scheme_eval_h\n",
    "\n",
    "# print(scheme_eval(cpa.atomspace, \"(cog-get-atoms 'PredictiveImplicationScopeLink)\").decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| msg: 'Learning phase started. (1/10)'\n",
      "ic| msg: 'Interaction phase started. (1/10)'\n",
      "ic| msg: 'Accumulated reward during 1th epoch = 20'\n",
      "ic| msg: ('Action counter during 1th epoch:\n",
      "         '\n",
      "          'Counter({(ExecutionLink\n",
      "         '\n",
      "          '  (SchemaNode \"Go Left\") ; [7ca1f8f2efc2f84d][2]\n",
      "         '\n",
      "          ') ; [ecf3179b1d921593][2]: 12, (ExecutionLink\n",
      "         '\n",
      "          '  (SchemaNode \"Go Right\") ; [51c74c8fd94d22b3][2]\n",
      "         '\n",
      "          ') ; [9073984dcee164b1][2]: 8})')\n",
      "ic| msg: 'Learning phase started. (2/10)'\n",
      "ic| msg: 'Interaction phase started. (2/10)'\n",
      "ic| msg: 'Accumulated reward during 2th epoch = 15'\n",
      "ic| msg: ('Action counter during 2th epoch:\n",
      "         '\n",
      "          'Counter({(ExecutionLink\n",
      "         '\n",
      "          '  (SchemaNode \"Go Left\") ; [7ca1f8f2efc2f84d][2]\n",
      "         '\n",
      "          ') ; [ecf3179b1d921593][2]: 10, (ExecutionLink\n",
      "         '\n",
      "          '  (SchemaNode \"Go Right\") ; [51c74c8fd94d22b3][2]\n",
      "         '\n",
      "          ') ; [9073984dcee164b1][2]: 5})')\n",
      "ic| msg: 'Learning phase started. (3/10)'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-39b811c6f415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Learning phase: discover patterns to make more informed decisions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlog_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Learning phase started. ({i + 1}/{epochs})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Run agent to accumulate percepta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/rocca/agents/core.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 )\n\u001b[1;32m    192\u001b[0m                 \u001b[0magent_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pos_multi_srps = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_multi_srps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 pos_multi_prdi = self.surprises_to_predictive_implications(\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mpos_multi_srps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 )\n",
      "\u001b[0;32m/workspace/rocca/agents/core.py\u001b[0m in \u001b[0;36msurprises_to_predictive_implications\u001b[0;34m(self, srps)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;31m# Turn patterns into predictive implication scopes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         cogscms = [\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_predictive_implication_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msrp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         ]\n",
      "\u001b[0;32m/workspace/rocca/agents/core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;31m# Turn patterns into predictive implication scopes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         cogscms = [\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_predictive_implication_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msrp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         ]\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/rocca/agents/core.py\u001b[0m in \u001b[0;36mto_predictive_implication_scope\u001b[0;34m(self, pattern)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# Calculate the truth value of the predictive implication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpln_bc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreimp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_desirable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcogscm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/rocca/agents/core.py\u001b[0m in \u001b[0;36mpln_bc\u001b[0;34m(self, query, maxiter)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m#:jobs {multiprocessing.cpu_count()})\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscheme_eval_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tb_writer = SummaryWriter(comment=\"cartpole-learning\")\n",
    "ac_logger.setLevel(logging.DEBUG)  # The agents.core logger\n",
    "\n",
    "agent = LearningCartPoleAgent(wrapped_env, log_level=\"fine\")\n",
    "agent.delta = 1.0e-16\n",
    "\n",
    "epochs = 10  # Number of epochs (learning / interacting episodes)\n",
    "epoch_len = 200\n",
    "\n",
    "for i in range(epochs):\n",
    "    wrapped_env.restart()\n",
    "    agent.reset_action_counter()\n",
    "    accreward = agent.accumulated_reward  # Keep track of the reward before\n",
    "\n",
    "    # Learning phase: discover patterns to make more informed decisions\n",
    "    log_msg(agent_log, f\"Learning phase started. ({i + 1}/{epochs})\")\n",
    "    agent.learn()\n",
    "    \n",
    "    # Run agent to accumulate percepta\n",
    "    log_msg(agent_log, f\"Interaction phase started. ({i + 1}/{epochs})\")\n",
    "    for j in range(epoch_len):\n",
    "        done = agent.step()\n",
    "        time.sleep(0.01)\n",
    "        log.debug(\"step_count = {}\".format(agent.step_count))\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    new_reward = agent.accumulated_reward - accreward\n",
    "    tb_writer.add_scalar(\"train/accumulated_reward\", new_reward, agent.step_count)\n",
    "    log_msg(agent_log, \"Accumulated reward during {}th epoch = {}\".format(i + 1, new_reward))\n",
    "    log_msg(agent_log, \"Action counter during {}th epoch:\\n{}\".format(i + 1, agent.action_counter))  # TODO: make the action counter look good\n",
    "\n",
    "log_msg(agent_log, f\"The average total reward over {epochs} trials (training): {agent.accumulated_reward / epochs}.\")\n",
    "# TODO: add a separate testing loop and measure average total reward."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
