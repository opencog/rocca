{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53cd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp agents.cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e499e02",
   "metadata": {},
   "source": [
    "# Cartpole Agent\n",
    "> An agent for solving the Cartpole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b87071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import gym\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# OpenCog\n",
    "from opencog.logger import log\n",
    "from opencog.pln import *\n",
    "from opencog.type_constructors import *\n",
    "from opencog.utilities import set_default_atomspace\n",
    "\n",
    "# ROCCA\n",
    "from rocca.envs.wrappers import GymWrapper\n",
    "from rocca.agents import OpencogAgent\n",
    "from rocca.agents.utils import *\n",
    "\n",
    "from rocca.utils import *\n",
    "from rocca.agents.core import logger as ac_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a384de5",
   "metadata": {},
   "source": [
    "## CartPole Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CartPoleWrapper(GymWrapper):\n",
    "    def __init__(self, env):\n",
    "        action_list = [\"Go Left\", \"Go Right\"]\n",
    "        super().__init__(env, action_list)\n",
    "\n",
    "    def labeled_observations(self, space, obs, sbs=\"\"):\n",
    "        \"\"\"Translate gym observation to Atomese\n",
    "\n",
    "        There are 4 observations (taken from CartPoleEnv help)\n",
    "\n",
    "        Observation               Min             Max\n",
    "        -----------               ---             ---\n",
    "        Cart Position             -4.8            4.8\n",
    "        Cart Velocity             -Inf            Inf\n",
    "        Pole Angle                -24 deg         24 deg\n",
    "        Pole Velocity At Tip      -Inf            Inf\n",
    "\n",
    "        They are represented in atomese as follows\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Cart Position\"\n",
    "            Number CP\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Cart Velocity\"\n",
    "            Number CV\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Pole Angle\"\n",
    "            Number PA\n",
    "\n",
    "        Evaluation\n",
    "            Predicate \"Pole Velocity At Tip\"\n",
    "            Number PVAT\n",
    "\n",
    "        Note that the observations are neither tv-set nor\n",
    "        timestamped. It is up to the caller to do it.\n",
    "\n",
    "        A python list (not an atomese list) is returned with these 4\n",
    "        Atomese observations.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        cp = NumberNode(str(obs[0]))\n",
    "        cv = NumberNode(str(obs[1]))\n",
    "        pa = NumberNode(str(obs[2]))\n",
    "        pvat = NumberNode(str(obs[3]))\n",
    "\n",
    "        return [\n",
    "            EvaluationLink(PredicateNode(\"Cart Position\"), cp),\n",
    "            EvaluationLink(PredicateNode(\"Cart Velocity\"), cv),\n",
    "            EvaluationLink(PredicateNode(\"Pole Angle\"), pa),\n",
    "            EvaluationLink(PredicateNode(\"Cart Velocity At Tip\"), pvat),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea6b7a",
   "metadata": {},
   "source": [
    "## Agent Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb01d4",
   "metadata": {},
   "source": [
    "### Fixed Rule Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class FixedCartPoleAgent(OpencogAgent):\n",
    "    def __init__(self, env: GymWrapper):\n",
    "        # Create Action Space. The set of allowed actions an agent can take.\n",
    "        # TODO take care of action parameters.\n",
    "        action_space = {ExecutionLink(SchemaNode(a)) for a in env.action_list}\n",
    "\n",
    "        # Create Goal\n",
    "        pgoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"1\"))\n",
    "        ngoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"0\"))\n",
    "\n",
    "        # Call super ctor\n",
    "        super().__init__(env, action_space, pgoal, ngoal)\n",
    "\n",
    "    def plan(self, goal, expiry) -> List:\n",
    "        \"\"\"Plan the next actions given a goal and its expiry time offset\n",
    "\n",
    "        Return a python list of cognitive schematics.  Whole cognitive\n",
    "        schematics are output (instead of action plans) in order to\n",
    "        make a decision based on their truth values.  Alternatively it\n",
    "        could return a pair (action plan, tv), where tv has been\n",
    "        evaluated to take into account the truth value of the context\n",
    "        as well (which would differ from the truth value of rule in\n",
    "        case the context is uncertain).\n",
    "\n",
    "        The format for a cognitive schematic is as follows\n",
    "\n",
    "        PredictiveImplicationScope <tv>\n",
    "          <vardecl>\n",
    "          <expiry>\n",
    "          And (or SimultaneousAnd?)\n",
    "            <context>\n",
    "            Execution\n",
    "              <action>\n",
    "              <input> [optional]\n",
    "              <output> [optional]\n",
    "          <goal>\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # For now we provide 2 hardwired rules\n",
    "        #\n",
    "        # 1. Push cart to the left (0) if angle is negative\n",
    "        # 2. Push cart to the right (1) if angle is positive\n",
    "        #\n",
    "        # with some arbitrary truth value (stv 0.9, 0.1)\n",
    "        angle = VariableNode(\"$angle\")\n",
    "        numt = TypeNode(\"NumberNode\")\n",
    "        time_offset = to_nat(1)\n",
    "        pole_angle = PredicateNode(\"Pole Angle\")\n",
    "        go_right = SchemaNode(\"Go Right\")\n",
    "        go_left = SchemaNode(\"Go Left\")\n",
    "        reward = PredicateNode(\"Reward\")\n",
    "        epsilon = NumberNode(\"0.01\")\n",
    "        mepsilon = NumberNode(\"-0.01\")\n",
    "        unit = NumberNode(\"1\")\n",
    "        hTV = TruthValue(0.9, 0.1)  # High TV\n",
    "        lTV = TruthValue(0.1, 0.1)  # Low TV\n",
    "\n",
    "        # PredictiveImplicationScope <high TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       Variable \"$angle\"\n",
    "        #       0\n",
    "        #     Execution\n",
    "        #       Schema \"Go Right\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_rr = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(angle, epsilon),\n",
    "                # Action\n",
    "                ExecutionLink(go_right),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=hTV,\n",
    "        )\n",
    "\n",
    "        # PredictiveImplicationScope <high TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       0\n",
    "        #       Variable \"$angle\"\n",
    "        #     Execution\n",
    "        #       Schema \"Go Left\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_ll = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(mepsilon, angle),\n",
    "                # Action\n",
    "                ExecutionLink(go_left),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=hTV,\n",
    "        )\n",
    "\n",
    "        # To cover all possibilities we shouldn't forget the complementary\n",
    "        # actions, i.e. going left when the pole is falling to the right\n",
    "        # and such, which should make the situation worse.\n",
    "\n",
    "        # PredictiveImplicationScope <low TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And (or SimultaneousAnd?)\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       Variable \"$angle\"\n",
    "        #       0\n",
    "        #     Execution\n",
    "        #       Schema \"Go Left\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_rl = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(angle, epsilon),\n",
    "                # Action\n",
    "                ExecutionLink(go_left),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=lTV,\n",
    "        )\n",
    "\n",
    "        # PredictiveImplicationScope <low TV>\n",
    "        #   TypedVariable\n",
    "        #     Variable \"$angle\"\n",
    "        #     Type \"NumberNode\"\n",
    "        #   Time \"1\"\n",
    "        #   And (or SimultaneousAnd?)\n",
    "        #     Evaluation\n",
    "        #       Predicate \"Pole Angle\"\n",
    "        #       Variable \"$angle\"\n",
    "        #     GreaterThan\n",
    "        #       0\n",
    "        #       Variable \"$angle\"\n",
    "        #     Execution\n",
    "        #       Schema \"Go Right\"\n",
    "        #   Evaluation\n",
    "        #     Predicate \"Reward\"\n",
    "        #     Number \"1\"\n",
    "        cs_lr = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(mepsilon, angle),\n",
    "                # Action\n",
    "                ExecutionLink(go_right),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=lTV,\n",
    "        )\n",
    "\n",
    "        # Ideally we want to return only relevant cognitive schematics\n",
    "        # (i.e. with contexts probabilistically currently true) for\n",
    "        # now however we return everything and let to the deduction\n",
    "        # process deal with it, as it should be able to.\n",
    "        return [cs_ll, cs_rr, cs_rl, cs_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_with(agent, knowledge):  # TODO: figure out how to pass atoms to be inserted.\n",
    "    set_default_atomspace(agent.atomspace)\n",
    "\n",
    "    angle = VariableNode(\"$angle\")\n",
    "    numt = TypeNode(\"NumberNode\")\n",
    "    time_offset = to_nat(1)\n",
    "    pole_angle = PredicateNode(\"Pole Angle\")\n",
    "    go_right = SchemaNode(\"Go Right\")\n",
    "    go_left = SchemaNode(\"Go Left\")\n",
    "    reward = PredicateNode(\"Reward\")\n",
    "    epsilon = NumberNode(\"0.01\")\n",
    "    mepsilon = NumberNode(\"-0.01\")\n",
    "    unit = NumberNode(\"1\")\n",
    "    hTV = TruthValue(0.9, 0.1)  # High TV\n",
    "    lTV = TruthValue(0.1, 0.1)  # Low TV\n",
    "\n",
    "    cs_rr = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(angle, epsilon),\n",
    "                # Action\n",
    "                ExecutionLink(go_right),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=hTV,\n",
    "        )\n",
    "\n",
    "    cs_ll = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(mepsilon, angle),\n",
    "                # Action\n",
    "                ExecutionLink(go_left),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=hTV,\n",
    "        )\n",
    "\n",
    "    cs_rl = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(angle, epsilon),\n",
    "                # Action\n",
    "                ExecutionLink(go_left),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=lTV,\n",
    "        )\n",
    "        \n",
    "    cs_lr = PredictiveImplicationScopeLink(\n",
    "            TypedVariableLink(angle, numt),\n",
    "            time_offset,\n",
    "            AndLink(\n",
    "                # Context\n",
    "                EvaluationLink(pole_angle, angle),\n",
    "                GreaterThanLink(mepsilon, angle),\n",
    "                # Action\n",
    "                ExecutionLink(go_right),\n",
    "            ),\n",
    "            # Goal\n",
    "            EvaluationLink(reward, unit),\n",
    "            # TV\n",
    "            tv=lTV,\n",
    "        )\n",
    "\n",
    "    agent.cognitive_schematics.update(set([cs_ll, cs_rr, cs_rl, cs_lr]))  # TODO: the code should update Python-side automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc670e",
   "metadata": {},
   "source": [
    "### Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class LearningCartPoleAgent(OpencogAgent):\n",
    "    def __init__(self, env: GymWrapper, log_level=\"debug\"):\n",
    "        # Create Action Space. The set of allowed actions an agent can take.\n",
    "        # TODO take care of action parameters.\n",
    "        action_space = {ExecutionLink(SchemaNode(a)) for a in env.action_list}\n",
    "\n",
    "        # Create Goal\n",
    "        pgoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"1\"))\n",
    "        ngoal = EvaluationLink(PredicateNode(\"Reward\"), NumberNode(\"0\"))\n",
    "\n",
    "        # Call super ctor\n",
    "        super().__init__(env, action_space, pgoal, ngoal, log_level=log_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c157da",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36009d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74752191",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcbdb7",
   "metadata": {},
   "source": [
    "### Fixed Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomspace = AtomSpace()\n",
    "set_default_atomspace(atomspace)\n",
    "wrapped_env = CartPoleWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20447aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb_writer = SummaryWriter(comment=\"cartpole-fixed\")\n",
    "\n",
    "cpa = FixedCartPoleAgent(wrapped_env)\n",
    "cpa.delta = 1.0e-16\n",
    "\n",
    "# Run control loop\n",
    "while not cpa.step():\n",
    "    time.sleep(0.1)\n",
    "    log.debug(\"step_count = {}\".format(cpa.step_count))\n",
    "    # tb_writer.add_scalar(\n",
    "    #     \"accumulated_reward\", cpa.accumulated_reward, cpa.step_count\n",
    "    # )\n",
    "\n",
    "log_msg(agent_log, f\"The final reward is {cpa.accumulated_reward}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpa.atomspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bcb0e9",
   "metadata": {},
   "source": [
    "### Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ac3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomspace = AtomSpace()\n",
    "set_default_atomspace(atomspace)\n",
    "wrapped_env = CartPoleWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb507364",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LearningCartPoleAgent(wrapped_env, log_level=\"fine\")\n",
    "agent.delta = 1.0e-16\n",
    "\n",
    "seed_with(agent, [\"fixme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ab4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter(comment=\"cartpole-learning-seeded\")\n",
    "ac_logger.setLevel(logging.DEBUG)  # The agents.core logger\n",
    "\n",
    "epochs = 10  # Number of epochs (learning / interacting episodes)\n",
    "epoch_len = 200\n",
    "\n",
    "for i in range(epochs):\n",
    "    wrapped_env.restart()\n",
    "    agent.reset_action_counter()\n",
    "    accreward = agent.accumulated_reward  # Keep track of the reward before\n",
    "\n",
    "    # Learning phase: discover patterns to make more informed decisions\n",
    "    log_msg(agent_log, f\"Learning phase started. ({i + 1}/{epochs})\")\n",
    "    agent.learn()\n",
    "    \n",
    "    # Run agent to accumulate percepta\n",
    "    log_msg(agent_log, f\"Interaction phase started. ({i + 1}/{epochs})\")\n",
    "    for j in range(epoch_len):\n",
    "        done = agent.step()\n",
    "        time.sleep(0.01)\n",
    "        log.debug(\"step_count = {}\".format(agent.step_count))\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    new_reward = agent.accumulated_reward - accreward\n",
    "    tb_writer.add_scalar(\"train/accumulated_reward\", new_reward, agent.step_count)\n",
    "    log_msg(agent_log, \"Accumulated reward during {}th epoch = {}\".format(i + 1, new_reward))\n",
    "    log_msg(agent_log, \"Action counter during {}th epoch:\\n{}\".format(i + 1, agent.action_counter))  # TODO: make the action counter look good\n",
    "\n",
    "log_msg(agent_log, f\"The average total reward over {epochs} trials (training): {agent.accumulated_reward / epochs}.\")\n",
    "# TODO: add a separate testing loop and measure average total reward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
