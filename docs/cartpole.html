---

title: Cartpole Agent


keywords: fastai
sidebar: home_sidebar

summary: "An agent for solving the Cartpole environment"
description: "An agent for solving the Cartpole environment"
nb_path: "01_cartpole.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_cartpole.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorboardX</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="CartPole-Wrapper">CartPole Wrapper<a class="anchor-link" href="#CartPole-Wrapper"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CartPoleWrapper" class="doc_header"><code>class</code> <code>CartPoleWrapper</code><a href="https://github.com/opencog/rocca/tree/master/rocca/agents/cartpole.py#L29" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CartPoleWrapper</code>(<strong><code>env</code></strong>) :: <code>GymWrapper</code></p>
</blockquote>
<p>This wrapper abstracts and decorates components of environments in order to
run agents on any environment. It is supposed to be derived by concrete
environment wrappers with the following specifications.</p>
<p>1| Restart method is supposed to begin a new environment session and return
   the initial reward, observation and environment state (is the environment
   done?).</p>
<p>2| Step method is going to take an action and return a reward, observation
   and environment state.</p>
<p>3| Close method to clean up env.</p>
<p>4| Actions have name and value.
   i:e
      ExecutionLink
        SchemaNode <action_name>
        NumberNode <value>
   NOTE: We are assuming the names of actions in the agent and the names of
         actions in the environment are in accordance. I am not sure if this
         is the right way to do it though.</p>
<p>5| Reward is represented as
   i:e
      EvaluationLink
        PredicateNode "Reward"
        NumberNode <reward></p>
<p>6| Observations are stored in a python list.
   i:e
      [
        EvaluationLink
            PredicateNode <observation_name>
            &lt;observation/s&gt;
        , ...
      ]</p>
<p>7| Environment state is <code>True</code> if environments' session has ended.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Agent-Definition">Agent Definition<a class="anchor-link" href="#Agent-Definition"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Fixed-Rule-Agent">Fixed Rule Agent<a class="anchor-link" href="#Fixed-Rule-Agent"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>???</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FixedCartPoleAgent" class="doc_header"><code>class</code> <code>FixedCartPoleAgent</code><a href="https://github.com/opencog/rocca/tree/master/rocca/agents/cartpole.py#L87" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FixedCartPoleAgent</code>(<strong><code>env</code></strong>:<code>GymWrapper</code>) :: <code>OpencogAgent</code></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learning-Agent">Learning Agent<a class="anchor-link" href="#Learning-Agent"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LearningCartPoleAgent" class="doc_header"><code>class</code> <code>LearningCartPoleAgent</code><a href="https://github.com/opencog/rocca/tree/master/rocca/agents/cartpole.py#L290" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LearningCartPoleAgent</code>(<strong><code>env</code></strong>:<code>GymWrapper</code>) :: <code>OpencogAgent</code></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Experiment">Experiment<a class="anchor-link" href="#Experiment"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Discrete(2)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">atomspace</span> <span class="o">=</span> <span class="n">AtomSpace</span><span class="p">()</span>
<span class="n">set_default_atomspace</span><span class="p">(</span><span class="n">atomspace</span><span class="p">)</span>
<span class="n">wrapped_env</span> <span class="o">=</span> <span class="n">CartPoleWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>  <span class="c1"># TODO: can I just use the `GymWrapper`?</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Fixed-Agent">Fixed Agent<a class="anchor-link" href="#Fixed-Agent"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tb_writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="s2">&quot;cartpole-fixed&quot;</span><span class="p">)</span>

<span class="n">cpa</span> <span class="o">=</span> <span class="n">FixedCartPoleAgent</span><span class="p">(</span><span class="n">wrapped_env</span><span class="p">)</span>
<span class="n">cpa</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="mf">1.0e-16</span>

<span class="c1"># Run control loop</span>
<span class="c1"># TODO: run over 100 trials</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">cpa</span><span class="o">.</span><span class="n">step</span><span class="p">():</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;step_count = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cpa</span><span class="o">.</span><span class="n">step_count</span><span class="p">))</span>
    <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;accumulated_reward&quot;</span><span class="p">,</span> <span class="n">cpa</span><span class="o">.</span><span class="n">accumulated_reward</span><span class="p">,</span> <span class="n">cpa</span><span class="o">.</span><span class="n">step_count</span><span class="p">)</span>

<span class="n">log_msg</span><span class="p">(</span><span class="n">agent_log</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;The final reward is </span><span class="si">{</span><span class="n">cpa</span><span class="o">.</span><span class="n">accumulated_reward</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>ic| msg: &#39;The final reward is 27.&#39;
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learning-Agent">Learning Agent<a class="anchor-link" href="#Learning-Agent"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tb_writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="s2">&quot;cartpole-learning&quot;</span><span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">LearningCartPoleAgent</span><span class="p">(</span><span class="n">wrapped_env</span><span class="p">)</span>
<span class="n">agent</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="mf">1.0e-16</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of epochs (learning / interacting episodes)</span>
<span class="n">epoch_len</span> <span class="o">=</span> <span class="mi">200</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">wrapped_env</span><span class="o">.</span><span class="n">restart</span><span class="p">()</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">reset_action_counter</span><span class="p">()</span>
    <span class="n">accreward</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">accumulated_reward</span>  <span class="c1"># Keep track of the reward before</span>

    <span class="c1"># Learning phase: discover patterns to make more informed decisions</span>
    <span class="n">log_msg</span><span class="p">(</span><span class="n">agent_log</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Learning phase started. (</span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>

    <span class="c1"># Run agent to accumulate percepta</span>
    <span class="n">log_msg</span><span class="p">(</span><span class="n">agent_log</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Interaction phase started. (</span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch_len</span><span class="p">):</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;step_count = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">step_count</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="n">new_reward</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">accumulated_reward</span> <span class="o">-</span> <span class="n">accreward</span>
    <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;train/accumulated_reward&quot;</span><span class="p">,</span> <span class="n">new_reward</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">step_count</span><span class="p">)</span>
    <span class="n">log_msg</span><span class="p">(</span>
        <span class="n">agent_log</span><span class="p">,</span> <span class="s2">&quot;Accumulated reward during </span><span class="si">{}</span><span class="s2">th epoch = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">new_reward</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">log_msg</span><span class="p">(</span>
        <span class="n">agent_log</span><span class="p">,</span>
        <span class="s2">&quot;Action counter during </span><span class="si">{}</span><span class="s2">th epoch:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">action_counter</span><span class="p">),</span>
    <span class="p">)</span>  <span class="c1"># TODO: make the action counter look good</span>

<span class="n">log_msg</span><span class="p">(</span><span class="n">agent_log</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;The average total reward over </span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2"> trials: </span><span class="si">{</span><span class="n">agent</span><span class="o">.</span><span class="n">accumulated_reward</span> <span class="o">/</span> <span class="n">epochs</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>ic| msg: &#39;Learning phase started. (1/10)&#39;
ic| msg: &#39;Interaction phase started. (1/10)&#39;
ic| msg: &#39;Accumulated reward during 1th epoch = 24&#39;
ic| msg: (&#39;Action counter during 1th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 12, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 12})&#39;)
ic| msg: &#39;Learning phase started. (2/10)&#39;
ic| msg: &#39;Interaction phase started. (2/10)&#39;
ic| msg: &#39;Accumulated reward during 2th epoch = 61&#39;
ic| msg: (&#39;Action counter during 2th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 33, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 28})&#39;)
ic| msg: &#39;Learning phase started. (3/10)&#39;
ic| msg: &#39;Interaction phase started. (3/10)&#39;
ic| msg: &#39;Accumulated reward during 3th epoch = 29&#39;
ic| msg: (&#39;Action counter during 3th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 17, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 12})&#39;)
ic| msg: &#39;Learning phase started. (4/10)&#39;
ic| msg: &#39;Interaction phase started. (4/10)&#39;
ic| msg: &#39;Accumulated reward during 4th epoch = 53&#39;
ic| msg: (&#39;Action counter during 4th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 30, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 23})&#39;)
ic| msg: &#39;Learning phase started. (5/10)&#39;
ic| msg: &#39;Interaction phase started. (5/10)&#39;
ic| msg: &#39;Accumulated reward during 5th epoch = 11&#39;
ic| msg: (&#39;Action counter during 5th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 8, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 3})&#39;)
ic| msg: &#39;Learning phase started. (6/10)&#39;
ic| msg: &#39;Interaction phase started. (6/10)&#39;
ic| msg: &#39;Accumulated reward during 6th epoch = 14&#39;
ic| msg: (&#39;Action counter during 6th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 10, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 4})&#39;)
ic| msg: &#39;Learning phase started. (7/10)&#39;
ic| msg: &#39;Interaction phase started. (7/10)&#39;
ic| msg: &#39;Accumulated reward during 7th epoch = 10&#39;
ic| msg: (&#39;Action counter during 7th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 10, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 0})&#39;)
ic| msg: &#39;Learning phase started. (8/10)&#39;
ic| msg: &#39;Interaction phase started. (8/10)&#39;
ic| msg: &#39;Accumulated reward during 8th epoch = 24&#39;
ic| msg: (&#39;Action counter during 8th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 17, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 7})&#39;)
ic| msg: &#39;Learning phase started. (9/10)&#39;
ic| msg: &#39;Interaction phase started. (9/10)&#39;
ic| msg: &#39;Accumulated reward during 9th epoch = 18&#39;
ic| msg: (&#39;Action counter during 9th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 11, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 7})&#39;)
ic| msg: &#39;Learning phase started. (10/10)&#39;
ic| msg: &#39;Interaction phase started. (10/10)&#39;
ic| msg: &#39;Accumulated reward during 10th epoch = 16&#39;
ic| msg: (&#39;Action counter during 10th epoch:
         &#39;
          &#39;Counter({(ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Left&#34;) ; [7ca1f8f2efc2f84d][2]
         &#39;
          &#39;) ; [ecf3179b1d921593][2]: 12, (ExecutionLink
         &#39;
          &#39;  (SchemaNode &#34;Go Right&#34;) ; [51c74c8fd94d22b3][2]
         &#39;
          &#39;) ; [9073984dcee164b1][2]: 4})&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

